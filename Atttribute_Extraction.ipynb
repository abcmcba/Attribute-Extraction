{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python 3.6\n",
    "!pip install tensorflow==1.13.1\n",
    "!pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk import word_tokenize\n",
    "from itertools import chain\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.engine import Layer, InputSpec\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import Sequential\n",
    "from keras_contrib.layers import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(label, string_chunk):\n",
    "    return [(word, label) for word in word_tokenize(string_chunk)]\n",
    "\n",
    "def one_hot_encoding(x, y, target = 'oem'):\n",
    "    y = np.array(y)\n",
    "    max_sentence_len = max(map(len, x))\n",
    "    print(max_sentence_len)\n",
    "    all_tags = set(chain(*y))\n",
    "    NUM_TAGS = len(all_tags)\n",
    "    TAGS_MAP = dict(zip(all_tags, range(NUM_TAGS)))\n",
    "    TAGS_MAP = {'0': 0, target: 1, 'NIL': 2}\n",
    "    y = list(map(lambda x: [TAGS_MAP[t] for t in x], y))\n",
    "    y = pad_sequences(y, max_sentence_len, padding='pre')\n",
    "    y = np.array([to_categorical(t, NUM_TAGS) for t in y])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPreparation:\n",
    "    def __init__(self, titles_filepath = None, default_n_classes=3):\n",
    "        self.default_n_classes = default_n_classes\n",
    "        self.titles_filepath = titles_filepath\n",
    "\n",
    "    def load_glove(self, glove_path = \"glove/glove.6B.50d.txt\"):\n",
    "        self.wordvecs = open(glove_path, encoding='utf-8')\n",
    "        self.word_to_ix_map = {}\n",
    "        for line in self.wordvecs:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            self.word_to_ix_map[word] = coefs\n",
    "        self.wordvecs.close()\n",
    "    \n",
    "    ## TODO: check for word sepration of special characters\n",
    "    def target_tagging(self, df):\n",
    "        t, attr = df.title, df[self.target]\n",
    "        start = (t.lower()).find(attr.lower())\n",
    "        end = start + len(attr)\n",
    "        labeled_title = labeling('0', t[:start]) + \\\n",
    "                    labeling(self.target, t[start:end]) + labeling('0', t[end:])\n",
    "        return labeled_title\n",
    "    \n",
    "    def read_data(self, df, target = 'oem'):\n",
    "        self.target = target\n",
    "        df['origin_title'] = df['title'].values\n",
    "        df['title'] = df.apply(self.target_tagging, axis=1)\n",
    "        self.df = df\n",
    "        raw_w, raw_t, raw_data, = [], [], []\n",
    "        for row in self.df.title:\n",
    "            for word, tag in row:\n",
    "                raw_w.append(word)\n",
    "                raw_t.append(tag)\n",
    "            raw_data.append((tuple(raw_w), tuple(raw_t)))\n",
    "            raw_w, raw_t = [], []\n",
    "        self.raw_data = raw_data\n",
    "\n",
    "    def prepare_data(self, d = 50): #d is dimension of word vectors\n",
    "        all_x, all_y = [], []\n",
    "        max_sentence_len = 49\n",
    "        for words, tags in self.raw_data:\n",
    "            encoded_words, encoded_tags = [], []\n",
    "            for w, t in zip(words, tags):\n",
    "                if w.lower() in self.word_to_ix_map:\n",
    "                    encoded_words.append(self.word_to_ix_map[w.lower()])\n",
    "                    encoded_tags.append(t)\n",
    "                else:\n",
    "                    encoded_words.append(np.ones(d))\n",
    "                    encoded_tags.append(t)\n",
    "            nil_x = np.zeros(d)\n",
    "            nil_y = 'NIL'\n",
    "            pad_length = max_sentence_len - len(encoded_words)\n",
    "            if pad_length<0:\n",
    "                print(pad_length)\n",
    "            all_x.append(np.array(((pad_length) * [nil_x]) + encoded_words))\n",
    "            all_y.append(np.array(((pad_length) * [nil_y]) + encoded_tags))\n",
    "        all_x, all_y = one_hot_encoding(all_x, all_y, self.target)\n",
    "        # all_x, all_y = np.array(all_x), np.array(all_y)\n",
    "        all_x= np.array(all_x)\n",
    "        return all_x, all_y\n",
    "\n",
    "    def train_test_split(self, all_x, all_y, test_size=0.2):\n",
    "        x_train, x_test, y_train, y_test, ind_train, ind_test = train_test_split(all_x, all_y, range(len(all_x)), test_size=test_size)\n",
    "        test_df = self.df[self.df.index.isin(ind_test)].reindex(ind_test).reset_index().origin_title\n",
    "        return x_train, x_test, y_train, y_test, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: train hyperparameter, fine tuning\n",
    "## TODO: augment train dataset with EM algorithm?\n",
    "class LstmAttributeDetector:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "\n",
    "    def create_model(self, dropout=0.5, units=150):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Bidirectional(LSTM(units, return_sequences=True),\n",
    "                                     input_shape=(49, 50)))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        self.model.add(Bidirectional(LSTM(units, return_sequences=True)))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        self.model.add(TimeDistributed(Dense(3)))\n",
    "        self.model.add(Dropout(dropout))\n",
    "        #crf = ChainCRF()\n",
    "        crf = CRF(3)  # CRF layer, n_tags+1(PAD)\n",
    "        # model.compile(optimizer=\"rmsprop\", loss=crf.loss_function, metrics=[crf.accuracy])\n",
    "        self.model.add(crf)\n",
    "        self.model.compile(loss=crf.loss_function, optimizer='nadam',\n",
    "                           metrics=['categorical_accuracy'])\n",
    "        \n",
    "\n",
    "    def fit(self, train_x, train_y, epochs=5, batch=100):\n",
    "        self.model.fit(train_x, train_y, epochs=epochs, batch_size=batch)\n",
    "\n",
    "    def save(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def print_summary(self):\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def predict(self, test_x):        \n",
    "        preds = self.model.predict(test_x, verbose=0).argmax(axis=-1)\n",
    "        return preds\n",
    "\n",
    "    def evaluate(self, test_x, test_y):  ## TODO look further into different train test split\n",
    "        y_pred = self.model.predict(test_x, verbose=0).argmax(axis=-1)\n",
    "        y_test = test_y.argmax(axis=-1)\n",
    "        acc = [np.array_equal(y_pred[i], y_test[i]) for i in\n",
    "               range(len(y_pred))].count(True) / len(y_pred)\n",
    "        return acc\n",
    "    \n",
    "#     def predict_output(self, test_x, test_df): ## TODO fix dimension not fit problem\n",
    "#         token_df = test_df.apply(word_tokenize)\n",
    "#         ind = self.model.predict(test_x, verbose=0).argmax(axis=-1)\n",
    "#         ind = [[z for z in obs if z!=2] for obs in ind]\n",
    "#         ind = [[False if elem == 0 else True for elem in obs] for obs in ind]\n",
    "#         output = [' '.join(np.array(token_df[i])[np.array(ind[i])]) for i in range(len(ind))]\n",
    "#         preds = pd.concat([test_df, pd.DataFrame(output, columns=['predictions'])], axis=1)\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading word embeddings...\n",
      "Reading data...\n",
      "Data preparing..\n",
      "49\n",
      "Model fitting...\n",
      "Epoch 1/8\n",
      "1931/1931 [==============================] - 18s 10ms/step - loss: 0.2503 - categorical_accuracy: 0.1031\n",
      "Epoch 2/8\n",
      "1931/1931 [==============================] - 8s 4ms/step - loss: 0.1524 - categorical_accuracy: 0.1031\n",
      "Epoch 3/8\n",
      "1931/1931 [==============================] - 9s 4ms/step - loss: 0.1346 - categorical_accuracy: 0.1031\n",
      "Epoch 4/8\n",
      "1931/1931 [==============================] - 8s 4ms/step - loss: 0.1174 - categorical_accuracy: 0.1031\n",
      "Epoch 5/8\n",
      "1931/1931 [==============================] - 9s 5ms/step - loss: 0.1036 - categorical_accuracy: 0.1031\n",
      "Epoch 6/8\n",
      "1931/1931 [==============================] - 9s 5ms/step - loss: 0.0950 - categorical_accuracy: 0.1031\n",
      "Epoch 7/8\n",
      "1931/1931 [==============================] - 9s 4ms/step - loss: 0.0834 - categorical_accuracy: 0.1031\n",
      "Epoch 8/8\n",
      "1931/1931 [==============================] - 9s 4ms/step - loss: 0.0759 - categorical_accuracy: 0.1031\n",
      "Accuracy for whole titles: 0.8633540372670807\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"phone.csv\") ## backmarket and uptradeit\n",
    "data = data.dropna()\n",
    "data = data.reset_index()\n",
    "prep = DataPreparation()\n",
    "print('Loading word embeddings...')\n",
    "prep.load_glove()\n",
    "print('Reading data...')\n",
    "prep.read_data(data, 'model_name')\n",
    "print('Data preparing..')\n",
    "x, y = prep.prepare_data()\n",
    "x_train, x_test, y_train, y_test, test_df = prep.train_test_split(x,y)\n",
    "model = LstmAttributeDetector()\n",
    "print('Model fitting...')\n",
    "model.create_model()\n",
    "model.fit(x_train, y_train, epochs=8)\n",
    "print('Accuracy for whole titles: {}'.format(model.evaluate(x_test, y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
